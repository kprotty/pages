---
layout: default
title: "Atomics and Memory Barriers"
description: "Getting this 'released' 'happened-before' I 'observed' any tpyos"
---

To start, this topic seems to often be poorly misunderstood, never understood, or hard to reason about even after understanding. I can firmly say here that I don't really know what im talking about. This is just how I understand these things. Take of it what you will. I'll also be using [LLVM's Atomic Memory Model](https://llvm.org/docs/Atomics.html) as a basis for everything.

## Atomics and shared memory

Lets get some definitions out of the way: 

* Parallelism is things happening at the same time (i.e. two people, each doing their own task). 
* Concurrency is multiple things happening, but not necessarily at the same time (i.e. one person switching between tasks)
* Atomicity is something appearing to happen in its entirety, with no observable partial states. (i.e. I appear to have written this blog atomically if you don't look at my commits)

In the face of parallel execution supported by hardware, atomic operations provide ways to do things completely without interruption. This is important for synchronization between CPU cores and even more so in a preemptive scheduling environment. In practice, there are 3 classes of operations that are made atomic by the hardware:

* **Loads**: reading de memory
* **Stores**: writing de memory
* **RMW (Read Modify Write)**: reading, doing, then writing de memory

For RMW operations, the base abstraction is called [Compare and Swap](https://en.wikipedia.org/wiki/Compare-and-swap) or `CAS` for short. It serves as a general purpose RMW operation in that all others can be implemented using it. Some pseudo code of it would look something along the lines of:

```py
CAS(memory, compare_with, swap_with):
    atomically:
        tmp = LOAD(memory)
        if tmp == compare_with:
            STORE(memory, swap_with)
            return OK
        else:
            return ERROR(tmp)
```

You can then build other RMW atomic operations on top of it:

```rs
// Atomically adds `add_amount` to memory.
// Returns the old value before the add took place.
FETCH_ADD(memory, add_amount):
    old = LOAD(memory)
    loop:
        new = old + add_amount
        match CAS(memory, old, new):
            if OK: return old
            if ERROR(updated): old = updated
```

### Bonus: ABA Problem

Some CPUs like x86 implement CAS using a [single instruction](https://www.felixcloutier.com/x86/cmpxchg). Other CPUs like ARM, RISCV, and MIPS implement CAS using a technique called [Load-link Store-conditional](https://en.wikipedia.org/wiki/Load-link/store-conditional) or `LL/SC` for short. In this scheme, it links an atomic load of memory with a conditional store to it. Its conditional in that: if another parallel store happened after we did the load but before we did our store, then our store fails so we have to retry. A `FETCH_ADD(memory, 5)` under LL/SC would then look something like this:

```py
LOOP:
    LOAD memory into REG # load-linked - linked with store below
    REG = REG + 5
    STORE REG into memory # store-conditional
    IF memory was updated before STORE: goto LOOP
```

This distinction is important because, in data structures which use atomics, you may want to update memory if 


----

## Memory visibility & soundness

## Memory reordering

## "Happens Before"

## Memory barriers

## Weakest constraint: Monotonic/Relaxed

## Release and Acquire

TODO: Ziggy mailbox illustration:
- Release:
    - Memory(`buffer[0] = 10`): writing the letter 
    - Release(`barrier(release)`): put in mailbox
    - Atomic(`store(ready, true)`): lift mailbox handle
- Acquire:
    - Atomic(`if load(ready)`): seeing mailbox handle lift
    - Acquire(`barrier(acquire)`): opening mailbox
    - Memory(`assert(buffer[0] == 10)`): reading the letter

## Release and Consume

## Sequential Consistency