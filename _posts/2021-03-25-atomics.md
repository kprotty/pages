---
layout: default
title: "What I wish I knew about Atomics and Memory"
description: "Getting this #released #happened-before I #observed any mistaeks"
---

Atomic operations and memory orderings are often misunderstood or never understood. Personally, this was the result of constrained explanations, misguided discussion, and the use of formal language that I couldn't easy reason with. My assumption is that these issues have bitten quite a few others as well when trying to learn about this stuff. 

The goal of this post is to present the model of how I reason about this topic in a way that would've helped me understand when first learning. If it helps you, cool. If not, whatever. Those looking for a full explanation should read the memory models for whatever they're targetting. In this case, I'll be going off of [LLVM's Atomic Memory Model](https://llvm.org/docs/Atomics.html).

# Atomics

First, lets get some definitions out of the way:

* **Parallelism**: things happening at the same time
    * Example: two people; each doing their own task. 
* **Concurrency**: multiple things happening, but not necessarily at the same time
    * Example: one person; switching between tasks
* **Atomicity**: things appearing to happen in their entirety with no observable partial states.
    * Example: I appear to have written this blog atomically if you don't look at my commits.

Good? Ok.

## Shared Memory

In the age where CPUs are hitting sequential compute gains, many have looked into [superscalar](https://en.wikipedia.org/wiki/Superscalar_processor) compute: using multiple distinct compute units. These don't necessarily have to be running in parallel. They can be concurrent like Intel's  [HyperThreading](https://www.intel.com/content/www/us/en/gaming/resources/hyper-threading.html). These little workers in the CPU may have to coordinate with each other after doing some work. They do this via by sharing memory. 

Shared memory is just what it sounds like: memory that is *"shared"* between multiple concurrent execution units. Thats a mouth-full so I'll be using CPU cores as a synonym instead. Anyway, these cores are specialized for doing work local to themselves since thats what how most work is. They need special methods for talking with each other if they need to coordinate or communicate via shared memory. This is where atomics come into play.

Atomic operations are an abstraction for cores to do things with shared memory in a correct and uninterruptable way. They can access shared memory with their local methods, but those may have different assumptions with how memory works which could lead to corrupting the shared memory. And if atomic operations were interruptable, they wouldn't have the "Atomicity" property as explained above which would make then effectively useless in a preemptive setting.

Preemption can be described as arbitrary concurrency. As in the ability for a core to concurrently pause a task to work on another one at an unspecified point in that task. This is the sort of mental model used when looking at OS threads: the kernel can preempt any thread at any moment to go run other stuff or another thread. Its also the mental model used for interrupts, which is a similar thing in the kernel but hardware preempts software. 

Reasoning about atomics for concurrent data structures is an interesting topic in and of itself. However what I want to focus on instead is what atomics as per LLVM represent, how to reason about them, and using it as a tool to coordinate and share memory correctly.

## Atomic Operations

I previously mentioned what atomic operations are, but not what they look like or how to use them. There are three main extensions to normal memory operations that are provided by atomics:

* **Loads**: reading from memory atomically
* **Stores**: writing to memory atomically
* **Read Modify Write (RMW)**: reading then writing to memory atomically

Loads and stores should be self explanatory I hope. If not, I suggest looking elsewhere for that info. Read Modify Write (or RMW for short) operations are the interesting ones here. They serve a the primary interface for synchronizing atomic operations as opposed to the core-mechanics of loading and storing.

Loads and stores have basically direct representations to their normal memory operation couterparts: you're already loading & storing memory but now its atomic. RMW operations are a little different to conceptualize. Atomic interfaces use an algorithm called [Compare And Swap](https://en.wikipedia.org/wiki/Compare-and-swap) (or CAS for short) to model this. It loads the memory, compares it with a given value, and stores a new value if the comparsion was equal - all atomically. Pseudo code is as follows:

```py
CAS(memory, compare_value, swap_value):
    atomically:
        current_value = LOAD(memory)
        if current_value == compare_value:
            STORE(memory, swap_value)
            return OK
        else:
            return ERROR(current_value)
```

This is a powerful abstraction. It's used to implement all sorts of concurrent data structures because of its versatility. If you have a CAS abstraction, you can implement all other RMW operations with it. For example:

```py
# atomically stores new_value to memory and returns its previous value
SWAP(memory, new_value):
    old_value = atomically: LOAD(memory)
    loop:
        result = CAS(memory, old_value, new_value)
        if result == OK:
            return old_value
        else if result == ERROR(updated):
            old_value = updated

# atomically adds amount to memory and returns the old value
FETCH_ADD(memory, amount):
    value = atomically: LOAD(memory)
    loop:
        result = CAS(memory, value, value + amount)
        if result == OK:
            return value
        else if result == ERROR(updated):
            value = updated
```

## Volatile

Now that I've gone over what tools are available, we should also talk what isn't one of them. There's a common misconception that the `volatile` keyword in LLVM-related languages is related to thread safey or atomicity in some way. I think this might've spawned from `volatile` in Java which makes all accesses to it [Sequentially Consistent](#Sequential-Consistency)? Either way, this isn't how it works here.

Volatile is instead used to stop the compiler from being clever with a memory operation. Optimizing compilers like LLVM are given the liberty to emit whatever machine code they want given it stays true to the programs declaration. This means that it can cache, move, and even remove entire memory operations as long as the code still does what you told it to (from a local program standpoint).

`volatile` tells the compiler that theres more going on here which it can't see so it should leave it alone when optimizing. An example where this is useful is memory-mapped IO. Reading/writing to an address could control hardware so you need to tell the compiler to leave those operations be even if it looks like they aren't needed. Another is in benchmarking where you want the result to be computed (to measure the code) even if you're not going to use the result. Optimizing compilers like to remove things you don't use... 

## Data Races

TODO

# Memory Ordering

TODO

## Superscalar Execution

TODO

## "Happens Before"

TODO

## Observability

TODO

## Cache Coherency

TODO

# Memory Barriers

TODO

## Unordered and Monotonic

TODO

## Release and Acquire

TODO

## Release and Consume

TODO

## Sequential Consistency

TODO

# Conclusion

TODO